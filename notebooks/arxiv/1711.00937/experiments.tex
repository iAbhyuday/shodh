\section{Experiments}\label{sec:exp}

\subsection{Comparison with continuous variables}

As a first experiment we compare VQ-VAE with normal VAEs (with continuous variables), as well as VIMCO \cite{vimco} with independent Gaussian or categorical priors. We train these models using the same standard VAE architecture on CIFAR10, while varying the latent capacity (number of continuous or discrete latent variables, as well as the dimensionality of the discrete space K). The encoder consists of 2 strided convolutional layers with stride 2 and window size $4\times 4$, followed by two residual $3\times 3$ blocks (implemented as ReLU, 3x3 conv, ReLU, 1x1 conv), all having 256 hidden units. The decoder similarly has two residual $3\times 3$ blocks, followed by two transposed convolutions with stride 2 and window size $4\times 4$. We use the ADAM optimiser \cite{kingma2014adam} with learning rate 2e-4 and evaluate the performance after 250,000 steps with batch-size 128. For VIMCO we use 50 samples in the multi-sample training objective.

The VAE, VQ-VAE and VIMCO models obtain \textbf{4.51} bits/dim, \textbf{4.67} bits/dim and \textbf{5.14} respectively. All reported likelihoods are lower bounds. Our numbers for the continuous VAE are comparable to those reported for a Deep convolutional VAE: \textbf{4.54} bits/dim \cite{gregor2016towards} on this dataset.

Our model is the first among those using discrete latent variables which challenges the performance of continuous VAEs. Thus, we get very good reconstructions like regular VAEs provide, with the compressed representation that symbolic representations provide. A few interesting characteristics, implications and applications of the VQ-VAEs that we train is shown in the next subsections.

\subsection{Images}

Images contain a lot of redundant information as most of the pixels are correlated and noisy, therefore learning models at the pixel level could be wasteful.

In this experiment we show that we can model $x=128\times128\times3$ images by compressing them to a $z=32\times32\times1$ discrete space (with K=512) via a purely deconvolutional $p(x|z)$. So a reduction of $\frac{128\times128\times3\times8}{32\times32\times9}\approx42.6$ in bits. We model images by learning a powerful prior (PixelCNN) over $z$. This allows to not only greatly speed up training and sampling, but also to use the PixelCNNs capacity to capture the global structure instead of the low-level statistics of images.

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{figures/imnet_orig_noborder.png}
\includegraphics[width=0.49\textwidth]{figures/imnet_recon_noborder.png}
\caption{Left: ImageNet 128x128x3 images, right: reconstructions from a VQ-VAE with a 32x32x1 latent space, with K=512.}
\label{fig:imnet_recon}
\end{figure}

Reconstructions from the 32x32x1 space with discrete latents are shown in Figure \ref{fig:imnet_recon}. Even considering that we greatly reduce the dimensionality with discrete encoding, the reconstructions look only slightly blurrier than the originals. It would be possible to use a more perceptual loss function than MSE over pixels here (e.g., a GAN \cite{goodfellow2014generative}), but we leave that as future work.

Next, we train a PixelCNN prior on the discretised 32x32x1 latent space. As we only have 1 channel (not 3 as with colours), we only have to use spatial masking in the PixelCNN. The capacity of the PixelCNN we used was similar to those used by the authors of the PixelCNN paper \cite{van2016conditional}. 

\begin{figure}[h]
\centering
\includegraphics[height=0.35\textwidth]{figures/kitfox_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/grey_whale_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/brown_bear_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/admiral_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/coral_reef_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/alp_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/microwave_noborder.png}
\includegraphics[height=0.35\textwidth]{figures/pickup_noborder.png}
\caption{Samples (128x128) from a VQ-VAE with a PixelCNN prior trained on ImageNet images. From left to right: kit fox, gray whale, brown bear, admiral (butterfly), coral reef, alp, microwave, pickup.}
\label{fig:imnet_samples}
\end{figure}

Samples drawn from the PixelCNN were mapped to pixel-space with the decoder of the VQ-VAE and can be seen in Figure \ref{fig:imnet_samples}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/lab.png}
\caption{Samples (128x128) from a VQ-VAE with a PixelCNN prior trained on frames captured from DeepMind Lab.}
\label{fig:lab_samples}
\end{figure}

We also repeat the same experiment for 84x84x3 frames drawn from the DeepMind Lab environment \cite{beattie2016deepmind}. The reconstructions looked nearly identical to their originals. Samples drawn from the PixelCNN prior trained on the 21x21x1 latent space and decoded to the pixel space using a deconvolutional model decoder can be seen in Figure \ref{fig:lab_samples}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/lab3latents_orig.png}
\includegraphics[width=\textwidth]{figures/lab3latents_sample.png}
\caption{Top original images, Bottom: reconstructions from a 2 stage VQ-VAE, with 3 latents to model the whole image (27 bits), and as such the model cannot reconstruct the images perfectly. The reconstructions are generated by sampled from the second PixelCNN prior in the 21x21 latent domain of first VQ-VAE, and then decoded with standard VQ-VAE decoder to 84x84. A lot of the original scene, including textures, room layout and nearby walls remain, but the model does not try to store the pixel values themselves, which means the textures are generated procedurally by the PixelCNN.}
\label{fig:lab_recon_samples}
\end{figure}

Finally, we train a second VQ-VAE with a \emph{PixelCNN decoder} on top of the 21x21x1 latent space from the first VQ-VAE on DM-LAB frames. This setup typically breaks VAEs as they suffer from "posterior collapse", i.e., the latents are ignored as the decoder is powerful enough to model $x$ perfectly. Our model however does not suffer from this, and the latents are meaningfully used. We use only three latent variables (each with K=512 and their own embedding space $e$) at the second stage for modelling the whole image and as such the model cannot reconstruct the image perfectly -- which is consequence of compressing the image onto 3 x 9 bits, i.e. less than a float32. Reconstructions sampled from the discretised global code can be seen in Figure \ref{fig:lab_recon_samples}.

\subsection{Audio}

\begin{figure}[h]
\centering
\includegraphics[width=0.32\textwidth]{figures/wav_orig.png}
\includegraphics[width=0.32\textwidth]{figures/wav_recon.png}
\includegraphics[width=0.32\textwidth]{figures/wav_transfer.png}
\caption{Left: original waveform, middle: reconstructed with same speaker-id, right: reconstructed with different speaker-id. The contents of the three waveforms are the same.}
\label{fig:audio}
\end{figure}

In this set of experiments we evaluate the behaviour of discrete latent variables on models of raw audio. In all our audio experiments, we train a VQ-VAE that has a dilated convolutional architecture similar to WaveNet decoder. All samples for this section can be played from the following url: \url{https://avdnoord.github.io/homepage/vqvae/}.

We first consider the VCTK dataset, which has speech recordings of 109 different speakers \cite{yamagishienglish}. We train a VQ-VAE where the encoder has 6 strided convolutions with stride 2 and window-size 4. This yields a latent space 64x smaller than the original waveform. The latents consist of one feature map and the discrete space is 512-dimensional. The decoder is conditioned on both the latents and a one-hot embedding for the speaker. 

First, we ran an experiment to show that VQ-VAE can extract a latent space that only conserves long-term relevant information. After training the model, given an audio example, we can encode it to the discrete latent representation, and reconstruct by sampling from the decoder. Because the dimensionality of the discrete representation is 64 times smaller, the original sample cannot be perfectly reconstructed sample by sample. As it can be heard from the provided samples, and as shown in Figure \ref{fig:action_lab}, the reconstruction has the same content (same text contents), but the waveform is quite different and prosody in the voice is altered. This means that the VQ-VAE has, without any form of linguistic supervision, learned a high-level abstract space that is invariant to low-level features and only encodes the content of the speech. This experiment confirms our observations from before that important features are often those that span many dimensions in the input data space (in this case phoneme and other high-level content in waveform).

We have then analysed the unconditional samples from the model to understand its capabilities. Given the compact and abstract latent representation extracted from the audio, we trained the prior on top of this representation to model the long-term dependencies in the data. For this task we have used a larger dataset of 460 speakers \cite{panayotov2015librispeech} and trained a VQ-VAE model where the resolution of discrete space is 128 times smaller. Next we trained the prior as usual on top of this representation on chunks of 40960 timesteps (2.56 seconds), which yields 320 latent timesteps. While samples drawn from even the best speech models like the original WaveNet \cite{van2016wavenet} sound like babbling , samples from VQ-VAE contain clear words and part-sentences (see samples linked above). We conclude that VQ-VAE was able to model a rudimentary phoneme-level language model in a completely unsupervised fashion from raw audio waveforms.

Next, we attempted the speaker conversion where the latents are extracted from one speaker and then reconstructed through the decoder using a separate speaker id. As can be heard from the samples, the synthesised speech has the same content as the original sample, but with the voice from the second speaker. This experiment again demonstrates that the encoded representation has factored out speaker-specific information: the embeddings not only have the same meaning regardless of details in the waveform, but also across different voice-characteristics.

Finally, in an attempt to better understand the content of the discrete codes we have compared the latents one-to-one with the ground-truth phoneme-sequence (which was not used any way to train the VQ-VAE). With a 128-dimensional discrete space that runs at $25$ Hz (encoder downsampling factor of $640$), we mapped every of the 128 possible latent values to one of the 41 possible phoneme values\footnote{Note that the encoder/decoder pairs could make the meaning of every discrete latent depend on previous latents in the sequence, e.g.. bi/tri-grams (and thus achieve a higher compression) which means a more advanced mapping to phonemes would results in higher accuracy.} (by taking the conditionally most likely phoneme). The accuracy of this 41-way classification was $49.3 \%$, while a random latent space would result in an accuracy of $7.2 \%$ (prior most likely phoneme). It is clear that these discrete latent codes obtained in a fully unsupervised way are high-level speech descriptors that are closely related to phonemes.

\subsection{Video}

For our final experiment we have used the DeepMind Lab \cite{beattie2016deepmind} environment to train a generative model conditioned on a given action sequence. In Figure~\ref{fig:action_lab} we show the initial $6$ frames that are input to the model followed by $10$ frames that are sampled from VQ-VAE with all actions set to \emph{forward} (top row) and \emph{right} (bottom row). Generation of the video sequence with the VQ-VAE model is done purely in the latent space, $z_t$ without the need to generate the actual images themselves. Each image in the sequence $x_t$ is then created by mapping the latents with a deterministic decoder to the pixel space after all the latents are generated using only the prior model $p(z_1, \ldots, z_T)$. Therefore, VQ-VAE can be used to imagine long sequences purely in latent space without resorting to pixel space. It can be seen that the model has learnt to successfully generate a sequence of frames conditioned on given action without any degradation in the visual quality whilst keeping the local geometry correct. For completeness, we trained a model without actions and obtained similar results, not shown due to space constraints.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/video_figure.png}
\caption{First 6 frames are provided to the model, following frames are generated conditioned on an action. Top: repeated action "move forward", bottom: repeated action "move right".}
\label{fig:action_lab}
\end{figure}