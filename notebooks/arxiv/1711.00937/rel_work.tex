\section{Related Work}\label{sec:relwork}

In this work we present a new way of training variational autoencoders \cite{kingma2013auto, rezende2014stochastic} with discrete latent variables \cite{mnih2014neural}. Using discrete variables in deep learning has proven challenging, as suggested by the dominance of continuous latent variables in most of current work -- even when the underlying modality is inherently discrete.

There exist many alternatives for training discrete VAEs. The NVIL \cite{mnih2014neural} estimator use a single-sample objective to optimise the variational lower bound, and uses various variance-reduction techniques to speed up training. VIMCO \cite{vimco} optimises a multi-sample objective \cite{burda2015importance}, which speeds up convergence further by using multiple samples from the inference network. 

Recently a few authors have suggested the use of a new continuous reparemetrisation based on the so-called Concrete \cite{maddison2016concrete} or Gumbel-softmax \cite{jang2016categorical} distribution, which is a continuous distribution and has a temperature constant that can be annealed during training to converge to a discrete distribution in the limit. In the beginning of training the variance of the gradients is low but biased, and towards the end of training the variance becomes high but unbiased. 

None of the above methods, however, close the performance gap of VAEs with continuous latent variables where one can use the Gaussian reparameterisation trick which benefits from much lower variance in the gradients. Furthermore, most of these techniques are typically evaluated on relatively small datasets such as MNIST, and the dimensionality of the latent distributions is small (e.g., below 8). In our work, we use three complex image datasets (CIFAR10, ImageNet, and DeepMind Lab) and a raw speech dataset (VCTK).

Our work also extends the line of research where autoregressive distributions are used in the decoder of VAEs and/or in the prior \cite{gregor2013deep}. This has been done for language modelling with LSTM decoders \cite{bowman2015generating}, and more recently with dilated convolutional decoders \cite{improvedtextvae}. PixelCNNs \cite{oord2016pixel, van2016conditional} are convolutional autoregressive models which have also been used as distribution in the decoder of VAEs \cite{pixelvae, chen2016variational}. 

Finally, our approach also relates to work in image compression with neural networks. Theis et. al. \cite{theis2017lossy} use scalar quantisation to compress activations for lossy image compression before arithmetic encoding. Other authors \cite{agustsson2017soft} propose a method for similar compression model with vector quantisation. The authors propose a continuous relaxation of vector quantisation which is annealed over time to obtain a hard clustering. In their experiments they first train an autoencoder, afterwards vector quantisation is applied to the activations of the encoder, and finally the whole network is fine tuned using the soft-to-hard relaxation with a small learning rate. In our experiments we were unable to train using the soft-to-hard relaxation approach from scratch as the decoder was always able to invert the continuous relaxation during training, so that no actual quantisation took place.
