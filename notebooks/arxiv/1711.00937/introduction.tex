\section{Introduction}\label{sec:intro}

Recent advances in generative modelling of images \cite{van2016conditional, goodfellow2014generative, gregor2016towards, kingma2016improved, dinh2016density}, audio \cite{van2016wavenet, mehri2016samplernn} and videos \cite{kalchbrenner2016video, finn2016unsupervised} have yielded impressive samples and applications \cite{ledig2016photo, isola2016image}. At the same time, challenging tasks such as few-shot learning \cite{santoro2016one}, domain adaptation \cite{hoffman2013efficient}, or reinforcement learning \cite{sutton1998reinforcement} heavily rely on learnt representations from raw data, but the usefulness of generic representations trained in an unsupervised fashion is still far from being the dominant approach.
 
Maximum likelihood and reconstruction error are two common objectives used to train unsupervised models in the pixel domain, however their usefulness depends on the particular application the features are used in. Our goal is to achieve a model that conserves the important features of the data in its latent space while optimising for maximum likelihood. As the work in \cite{chen2016variational} suggests, the best generative models (as measured by log-likelihood) will be those without latents but a powerful decoder (such as PixelCNN). However, in this paper, we argue for learning discrete and useful latent variables, which we demonstrate on a variety of domains.


Learning representations with continuous features have been the focus of many previous work \cite{hinton2006reducing,vincent2010stacked, infogan, denton2016semi} however we concentrate on discrete representations \cite{mnih2014neural,salakhutdinov2009deep,courville2011spike,vimco} which are potentially a more natural fit for many of the modalities we are interested in. Language is inherently discrete, similarly speech is typically represented as a sequence of symbols. Images can often be described concisely by language \cite{vinyals2015show}. Furthermore, discrete representations are a natural fit for complex reasoning,  planning and predictive learning (e.g., if it rains, I will use an umbrella). While using discrete latent variables in deep learning has proven challenging, powerful autoregressive models have been developed for modelling distributions over discrete variables \cite{van2016wavenet}.
 
In our work, we introduce a new family of generative models succesfully combining the variational autoencoder (VAE) framework with discrete  latent representations through a novel parameterisation of the posterior distribution of (discrete) latents given an observation. Our model, which relies on vector quantization (VQ), is simple to train, does not suffer from large variance, and avoids the ``posterior collapse’’ issue which has been problematic with many VAE models that have a powerful decoder, often caused by latents being ignored. Additionally, it is the first discrete latent VAE model that get similar performance as its continuous counterparts, while offering the flexibility of discrete distributions. We term our model the VQ-VAE.

Since VQ-VAE can make effective use of the latent space, it can successfully model important features that usually span many dimensions in data space (for example objects span many pixels in images, phonemes in speech, the message in a text fragment, etc.) as opposed to focusing or spending capacity on noise and imperceptible details which are often local.

Lastly, once a good discrete latent structure of a modality is discovered by the VQ-VAE, we train a powerful prior over these discrete random variables, yielding interesting samples and useful applications.
For instance, when trained on speech we discover the latent structure of language without any supervision or prior knowledge about phonemes or words. Furthermore, we can equip our decoder with the speaker identity, which allows for speaker conversion, i.e., transferring the voice from one speaker to another without changing the contents. We also show promising results on learning long term structure of environments for RL. 

Our contributions can thus be summarised as:
 
\begin{itemize}
\item Introducing the VQ-VAE model, which is simple, uses discrete latents, does not suffer from ``posterior collapse’’ and has no variance issues. 
\item We show that a discrete latent model (VQ-VAE) perform as well as its continuous model counterparts in log-likelihood.
\item When paired with a powerful prior, our samples are coherent and high quality on a wide variety of applications such as speech and video generation.
\item We show evidence of learning language through raw speech, without any supervision, and show applications of unsupervised speaker conversion.
\end{itemize}
