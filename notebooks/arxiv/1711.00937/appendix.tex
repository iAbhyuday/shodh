\newpage

\appendix 
\section{Appendix}

\subsection{VQ-VAE dictionary updates with Exponential Moving Averages}
\label{appendix:ema}

As mentioned in Section \ref{section:learning}, one can also use exponential moving averages (EMA) to update the dictionary items instead of the loss term from Equation \ref{eq_loss}:
\begin{equation}
\|\text{sg}[z_e(x)] - e\|^2_2.
\label{loss_dict}
\end{equation}

Let $\{z_{i, 1}, z_{i, 2}, \dots, z_{i, n_i}\}$ be the set of $n_i$ outputs from the encoder that are closest to dictionary item $e_i$, so that we can write the loss as:
\begin{equation}
\sum_j^{n_i} \|z_{i, j} - e_i\|^2_2.
\end{equation}
The optimal value for $e_i$ has a closed form solution, which is simply the average of elements in the set:
$$
e_i = \frac{1}{n_i}\sum_j^{n_i} z_{i,j}.
$$
This update is typically used in algorithms such as K-Means.

However, we cannot use this update directly when working with minibatches. Instead we can use exponential moving averages as an online version of this update:
\begin{align}
N^{(t)}_i &:= N^{(t-1)}_i * \gamma + n^{(t)}_i (1 - \gamma) \\
m^{(t)}_i &:= m^{(t-1)}_i * \gamma + \sum_j z^{(t)}_{i,j} (1 - \gamma) \\
e^{(t)}_i &:= \frac{m^{(t)}_i}{N^{(t)}_i}, \label{ema}
\end{align}
with $\gamma$ a value between 0 and 1. We found $\gamma=0.99$ to work well in practice.